Look for patterns in the .clinerules file related to FinBERT models and Gemma 3 implementations, particularly the 5-class sentiment classification. 

New patterns to document:
1. Gemma 3 model uses LoRA fine-tuning similar to Gamma 3 with r=8, alpha=16
2. All model implementations follow consistent notebook structure for better comparison
3. All models implement multi-metric early stopping for improved quality
4. The 5-class sentiment system is now standardized across all model implementations
5. All models are trained with similar hyperparameters for fair comparison
6. The naming convention for notebooks is consistent with 02a_ prefix for main model implementations
7. The project follows a strict notebook execution order for reproducible results
8. Data processing notebooks (00_*) handle different aspects of data preparation
9. Model training notebooks (02_*) handle different model architectures but share common patterns
10. Output files follow a consistent naming pattern and are organized in the data directory
11. Model artifacts follow a consistent structure in the models directory with separate subdirectories
12. The README.md in notebooks directory serves as the primary documentation for the notebooks
13. Dependencies are documented in the README.md with explicit version requirements
14. API key setup follows a consistent pattern with environment variables and key rotation
15. 8-bit quantization is used for efficient training of large models
16. The project name PARIS stands for Personalized AI-Advisor for Robo Investment Strategies
17. The project follows a Docker-based development workflow with standardized Makefile commands
18. Common model issues (Gemma 3 AttributeError, FinBERT column mismatch) are documented with fixes
19. Gemma 3 models use "model_dim" instead of "hidden_size" for configuration
20. The development environment provides consistent access points (localhost:3000, 8000, 8888)
21. Python scripts are provided alongside notebooks for command-line execution
22. The project has a clear structure with dedicated directories for data, models, notebooks, and web components
23. Multiple solutions are provided for common issues (standalone scripts, notebook modifications, automated fixes)
24. The main README.md provides comprehensive setup and usage instructions
25. Fix scripts follow a consistent naming pattern (fix_*_notebook.py)
26. The project includes additional directories not documented in the main README.md:
   - scraper/ - For Twitter data scraping
   - src/ - For core Python modules
   - logs/ - For training and application logs
   - example/ - For example implementations
   - memory-bank/ - For Cline documentation
   - tmp/ - For temporary files
27. The web directory has its own docker-compose.yml for web-specific container configuration
28. The project contains documentation beyond the main README.md, including TWITTER_API_README.md in the scraper directory
29. Project configuration is maintained through various dotfiles (.env, .gitignore, .gitmessage)
30. The scraper component is modular with its own requirements.txt for dependencies
31. Model comparison notebooks (03_*) provide detailed analysis and visualization of model performance
32. Prediction notebooks (04_*) focus on applying trained models to larger datasets
33. Batch processing is implemented for efficient prediction on large datasets
34. Output files use timestamp-based naming for clarity and version tracking
35. Analysis of prediction distributions is included in prediction notebooks
36. The standard prediction output includes confidence scores for each sentiment class 

# Backtesting Framework Patterns
37. The backtest directory contains scripts for backtesting sentiment predictions
38. The final_backtest.py script is the main implementation for sentiment-based strategy testing
39. Backtest scripts extract ticker symbols from the financial_info JSON-like field in predictions
40. The backtesting framework uses yfinance for fetching historical price data
41. Backtesting results are visualized with matplotlib in multiple chart types
42. The backtest output is organized in the results/ subdirectory with consistent naming
43. Trade details are saved in CSV format for each ticker (trades_TICKER.csv)
44. A summary of all backtest results is saved in backtest_summary.csv
45. The backtesting strategy uses a 3-day holding period for each sentiment signal
46. Positive sentiments (POSITIVE, STRONGLY_POSITIVE) generate BUY signals
47. Negative sentiments (NEGATIVE, STRONGLY_NEGATIVE) generate SELL signals
48. NEUTRAL sentiments are skipped in the trading strategy
49. Individual ticker analysis charts show both full period stats and trade performance
50. Sentiment analysis charts show performance by sentiment type
51. Timestamp parsing supports multiple formats including ISO 8601
52. The script includes extensive error handling for missing data and invalid inputs
53. The backtest script limits processing to top tickers by default to manage processing time
54. Performance metrics include both trade-specific metrics and full period statistics
55. The visualization includes color coding (green/red) based on positive/negative performance 
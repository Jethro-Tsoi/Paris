{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (4.4.0)\n",
      "Collecting typing_extensions\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.1.0+cu118)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: typing_extensions, bitsandbytes\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed bitsandbytes-0.45.4 typing_extensions-4.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting peft\n",
      "  Downloading peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu118)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\n",
      "Collecting safetensors (from peft)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.2.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.15.1-py3-none-any.whl (411 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.2/316.2 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m139.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: werkzeug, tensorboard-data-server, safetensors, regex, protobuf, markdown, grpcio, absl-py, tensorboard, tokenizers, accelerate, transformers, peft\n",
      "Successfully installed absl-py-2.2.1 accelerate-1.5.2 grpcio-1.71.0 markdown-3.7 peft-0.15.1 protobuf-6.30.2 regex-2024.11.6 safetensors-0.5.3 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tokenizers-0.21.1 transformers-4.50.3 werkzeug-3.1.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas numpy torch scikit-learn tqdm huggingface_hub\n",
    "!pip install -U bitsandbytes typing_extensions\n",
    "!pip install -U peft transformers accelerate tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Gemma 3 Model Training with LoRA\n",
    "\n",
    "This notebook implements the training pipeline for Google's Gemma 3 model using LoRA (Low-Rank Adaptation) for efficient fine-tuning.\n",
    "\n",
    "Features:\n",
    "1. LoRA implementation\n",
    "2. Multi-metric early stopping\n",
    "3. Evaluation metrics tracking\n",
    "4. Multimodal capabilities (text classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    cohen_kappa_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialTweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_metrics = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, metrics):\n",
    "        if self.best_metrics is None:\n",
    "            self.best_metrics = metrics\n",
    "            return False\n",
    "        \n",
    "        # Check if any metric improved by min_delta\n",
    "        improved = False\n",
    "        for metric, value in metrics.items():\n",
    "            if value > self.best_metrics[metric] + self.min_delta:\n",
    "                improved = True\n",
    "                self.best_metrics = metrics\n",
    "                break\n",
    "        \n",
    "        if not improved:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, labels):\n",
    "    \"\"\"Calculate multiple evaluation metrics\"\"\"\n",
    "    pred_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(labels, pred_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, pred_labels, average='weighted'\n",
    "    )\n",
    "    \n",
    "    # Additional metrics\n",
    "    kappa = cohen_kappa_score(labels, pred_labels)\n",
    "    mcc = matthews_corrcoef(labels, pred_labels)\n",
    "    \n",
    "    # ROC-AUC (multi-class)\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(labels, predictions, multi_class='ovr')\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'kappa': kappa,\n",
    "        'mcc': mcc,\n",
    "        'roc_auc': roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled data\n",
    "df = pd.read_csv('all_labeled_tweets.csv')\n",
    "\n",
    "# Convert labels to numeric\n",
    "label_map = {\n",
    "    'STRONGLY_POSITIVE': 0,\n",
    "    'POSITIVE': 1,\n",
    "    'NEUTRAL': 2,\n",
    "    'NEGATIVE': 3,\n",
    "    'STRONGLY_NEGATIVE': 4\n",
    "}\n",
    "df['label'] = df['sentiment'].map(label_map)\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['description'].values, df['label'].values,\n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma 3 Model Setup\n",
    "\n",
    "We'll be using the 12B parameter pretrained Gemma 3 model from Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197974d7b8e94924a52449edd22d4b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22f4920e64b40b698b03cd750bfc3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857f506a76974b9d8785af7e913e642c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ceb153f6334c1fa2928794c5acb3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed1a122152f496d87c29b420a572b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize tokenizer for Gemma 3\n",
    "model_name = \"google/gemma-3-4b-pt\"\n",
    "# Add Hugging Face authentication\n",
    "from huggingface_hub import login\n",
    "# Replace 'your_token_here' with your actual token or use environment variables\n",
    "# You can get a token from https://huggingface.co/settings/tokens\n",
    "import os\n",
    "hf_token = os.environ.get(\"HF_TOKEN\", None)\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n",
    "else:\n",
    "    print(\"⚠️ Hugging Face token not found! Please set the HF_TOKEN environment variable.\")\n",
    "    print(\"You need to log in to access Gemma 3, which is a gated model. Visit:\")\n",
    "    print(\"https://huggingface.co/google/gemma-3-4b-pt and accept the license\")\n",
    "    login()  # Interactive login if running in interactive environment\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FinancialTweetDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = FinancialTweetDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f37dca4a90447a9f96fa9b42020878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/815 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1682: FutureWarning: Loading a multimodal model with `AutoModelForCausalLM` is deprecated and will be removed in v5. `AutoModelForCausalLM` will be used to load only the text-to-text generation module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe11df9ee034869bab84b74eca2e569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cf3c96ed3b4e38b657aeb444895df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0b7128ffd64ad890c7a5a6f01a0554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec44b9f374e41b2ae02fc1ce49fd225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b21aa8b90147d1ba39c3e16870fe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d979e4dcacc444cae3c58479910e3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Using default hidden dimension of 262208 for Gemma 3 model\n",
      "Using hidden dimension: 262208\n",
      "Base model device: cuda:0, dtype: torch.bfloat16\n",
      "Moved classifier to device: cuda:0\n",
      "trainable params: 3,223,552 || all params: 4,304,614,069 || trainable%: 0.0749\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with LoRA config\n",
    "# Load in 8-bit precision to reduce memory requirements\n",
    "# Initialize model with LoRA config\n",
    "# Load in 8-bit precision to reduce memory requirements\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Gemma 3 doesn't work with AutoModelForSequenceClassification\n",
    "# Let's use AutoModelForCausalLM which works with Gemma 3 models\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=hf_token  # Pass the token for authentication\n",
    ")\n",
    "\n",
    "# Add a sequence classification head on top\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from torch import nn\n",
    "\n",
    "class GemmaForSequenceClassification(nn.Module):\n",
    "    def __init__(self, base_model, tokenizer, num_labels=5):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_labels = num_labels\n",
    "        self._config = base_model.config  # Store the config\n",
    "        \n",
    "        # Create the prompt template\n",
    "        self.prompt = \"\"\"You are a financial sentiment analyzer. Classify the given tweet's sentiment into one of these categories:\n",
    "\n",
    "                        STRONGLY_POSITIVE - Very bullish, highly confident optimistic outlook\n",
    "                        POSITIVE - Generally optimistic, bullish view\n",
    "                        NEUTRAL - Factual, balanced, or no clear sentiment\n",
    "                        NEGATIVE - Generally pessimistic, bearish view\n",
    "                        STRONGLY_NEGATIVE - Very bearish, highly confident pessimistic outlook\n",
    "\n",
    "                        Examples:\n",
    "                        \"Breaking: Company XYZ doubles profit forecast!\" -> STRONGLY_POSITIVE\n",
    "                        \"Expecting modest gains next quarter\" -> POSITIVE\n",
    "                        \"Market closed at 35,000\" -> NEUTRAL\n",
    "                        \"Concerned about rising rates\" -> NEGATIVE\n",
    "                        \"Crash incoming, sell everything!\" -> STRONGLY_NEGATIVE\n",
    "\n",
    "                        Format: Return only one word from: STRONGLY_POSITIVE, POSITIVE, NEUTRAL, NEGATIVE, STRONGLY_NEGATIVE\n",
    "\n",
    "                        Analyze the sentiment of this tweet: \"\"\"        \n",
    "        self.prompt_ids = tokenizer.encode(self.prompt, add_special_tokens=False)\n",
    "\n",
    "        # For Gemma 3, use model_dim instead of hidden_size\n",
    "        if hasattr(base_model.config, \"model_dim\"):\n",
    "            hidden_dim = base_model.config.model_dim\n",
    "        elif hasattr(base_model.config, \"hidden_size\"):\n",
    "            hidden_dim = base_model.config.hidden_size\n",
    "        else:\n",
    "            # Default value for Gemma 3 4B model is 262208\n",
    "            print(\"Warning: Using default hidden dimension of 262208 for Gemma 3 model\")\n",
    "            hidden_dim = 262208\n",
    "            \n",
    "        print(f\"Using hidden dimension: {hidden_dim}\")\n",
    "\n",
    "        # Get device and dtype from base model\n",
    "        device = next(base_model.parameters()).device\n",
    "        dtype = next(base_model.parameters()).dtype\n",
    "        print(f\"Base model device: {device}, dtype: {dtype}\")\n",
    "\n",
    "        # Create classifier with matching dtype and move to correct device\n",
    "        self.classifier = nn.Linear(hidden_dim, num_labels, dtype=dtype).to(device)\n",
    "        print(f\"Moved classifier to device: {device}\")\n",
    "\n",
    "    # Add a property to expose the config\n",
    "    @property\n",
    "    def config(self):\n",
    "        return self._config\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Prepend prompt to each input\n",
    "        prompt_ids_tensor = torch.tensor([self.prompt_ids] * batch_size, device=input_ids.device)\n",
    "        modified_input_ids = torch.cat([prompt_ids_tensor, input_ids], dim=1)\n",
    "        \n",
    "        # Adjust attention mask\n",
    "        prompt_attention = torch.ones(batch_size, len(self.prompt_ids), device=attention_mask.device)\n",
    "        modified_attention_mask = torch.cat([prompt_attention, attention_mask], dim=1)\n",
    "        \n",
    "        # Run the model with the prompt\n",
    "        outputs = self.base_model(\n",
    "            input_ids=modified_input_ids, \n",
    "            attention_mask=modified_attention_mask\n",
    "        )\n",
    "        \n",
    "        # For Gemma 3, handle different output structures\n",
    "        if hasattr(outputs, \"last_hidden_state\"):\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "        elif hasattr(outputs, \"hidden_states\") and outputs.hidden_states is not None:\n",
    "            # Use the last layer's hidden states if available\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "        else:\n",
    "            # For CausalLM models, we can often use the logits\n",
    "            # but first reshape them to get the hidden representation\n",
    "            # print(\"Using logits for classification - this might not give optimal results\")\n",
    "            hidden_states = outputs.logits\n",
    "        \n",
    "        # Use a pooled representation of the sequence after the prompt\n",
    "        # This combines the prompt context with the tweet content\n",
    "        prompt_length = len(self.prompt_ids)\n",
    "        \n",
    "        # Extract non-prompt tokens\n",
    "        relevant_states = hidden_states[:, prompt_length:, :]\n",
    "        relevant_mask = modified_attention_mask[:, prompt_length:]\n",
    "        \n",
    "        # Mean pooling over the relevant tokens\n",
    "        mask_expanded = relevant_mask.unsqueeze(-1).expand(relevant_states.size()).to(dtype=relevant_states.dtype)\n",
    "        sum_hidden = torch.sum(relevant_states * mask_expanded, 1)\n",
    "        count = torch.clamp(torch.sum(mask_expanded, 1), min=1e-9)  # Avoid division by zero\n",
    "        pooled_output = sum_hidden / count\n",
    "        \n",
    "        # Apply the classification head\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "            \n",
    "        class SequenceClassifierOutput:\n",
    "            def __init__(self, loss, logits):\n",
    "                self.loss = loss\n",
    "                self.logits = logits\n",
    "        \n",
    "        return SequenceClassifierOutput(loss, logits)\n",
    "    \n",
    "    # Add methods required for PEFT with CAUSAL_LM\n",
    "    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        This method is required by PEFT for CAUSAL_LM task type.\n",
    "        It delegates to the base model's method.\n",
    "        \"\"\"\n",
    "        return self.base_model.prepare_inputs_for_generation(*args, **kwargs)\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        \"\"\"Return the output embeddings from the base model if needed for generation\"\"\"\n",
    "        return self.base_model.get_output_embeddings()\n",
    "\n",
    "    # Forward all attribute requests that we don't handle to the base model\n",
    "    def __getattr__(self, name):\n",
    "        try:\n",
    "            return super().__getattr__(name)\n",
    "        except AttributeError:\n",
    "            # If we don't have the attribute, try getting it from base_model\n",
    "            return getattr(self.base_model, name)\n",
    "    \n",
    "# Wrap the model with our classification head\n",
    "model = GemmaForSequenceClassification(model, tokenizer, num_labels=5)  # 5 classes for the sentiment labels\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Target the attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM  # Changed from SEQ_CLS to CAUSAL_LM for Gemma 3\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters percentage\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test For Model dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Loading model with BF16 precision...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54ae27553d34c669fcb0aea5cc8a07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BF16 test succeeded!\n",
      "Output: Hello, how are you?\n",
      "\n",
      "I'm trying to use the <code>\n",
      "\n",
      "\n",
      "Testing working model with sentiment prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet: Breaking: Company XYZ doubles profit forecast!\n",
      "Model response: '\n",
      "\n",
      "Answer: STRONGLY_POSITIVE\n",
      "\n",
      "'\n",
      "\n",
      "After you find a working model configuration, update your model loading code accordingly.\n"
     ]
    }
   ],
   "source": [
    "# Try alternative loading configurations for the Gemma 3 model\n",
    "# First, make sure you have the necessary libraries\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "\n",
    "# Load the model with more stable settings\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Option 1: Try with BF16 precision instead of FP16\n",
    "print(\"Loading model with BF16 precision...\")\n",
    "try:\n",
    "    model_bf16 = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,  # Use bfloat16 instead of float16\n",
    "        device_map=\"auto\",\n",
    "        token=hf_token\n",
    "    )\n",
    "    \n",
    "    # Simple test\n",
    "    test_prompt = \"Hello, how are you?\"\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model_bf16.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_bf16.generate(\n",
    "            inputs.input_ids,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False  # Deterministic generation first\n",
    "        )\n",
    "    \n",
    "    print(\"BF16 test succeeded!\")\n",
    "    print(f\"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "    working_model = model_bf16\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"BF16 loading failed with error: {str(e)}\")\n",
    "    working_model = None\n",
    "\n",
    "# Option 2: Try with 4-bit quantization\n",
    "if working_model is None:\n",
    "    print(\"\\nTrying with 4-bit quantization...\")\n",
    "    try:\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float32,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        \n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\",\n",
    "            token=hf_token\n",
    "        )\n",
    "        \n",
    "        # Simple test\n",
    "        test_prompt = \"Hello, how are you?\"\n",
    "        inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model_4bit.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_4bit.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        print(\"4-bit quantization test succeeded!\")\n",
    "        print(f\"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "        working_model = model_4bit\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"4-bit loading failed with error: {str(e)}\")\n",
    "\n",
    "# Option 3: Try with 8-bit quantization\n",
    "if working_model is None:\n",
    "    print(\"\\nTrying with 8-bit quantization...\")\n",
    "    try:\n",
    "        model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            load_in_8bit=True,\n",
    "            device_map=\"auto\",\n",
    "            token=hf_token\n",
    "        )\n",
    "        \n",
    "        # Simple test\n",
    "        test_prompt = \"Hello, how are you?\"\n",
    "        inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model_8bit.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_8bit.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        print(\"8-bit quantization test succeeded!\")\n",
    "        print(f\"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "        working_model = model_8bit\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"8-bit loading failed with error: {str(e)}\")\n",
    "\n",
    "# Option 4: Try with full 32-bit precision (will use more memory)\n",
    "if working_model is None:\n",
    "    print(\"\\nTrying with full 32-bit precision...\")\n",
    "    try:\n",
    "        model_32bit = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,\n",
    "            device_map=\"auto\",\n",
    "            token=hf_token\n",
    "        )\n",
    "        \n",
    "        # Simple test\n",
    "        test_prompt = \"Hello, how are you?\"\n",
    "        inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model_32bit.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_32bit.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        print(\"32-bit precision test succeeded!\")\n",
    "        print(f\"Output: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "        working_model = model_32bit\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"32-bit loading failed with error: {str(e)}\")\n",
    "\n",
    "# If we found a working model configuration, now test it with our sentiment prompt\n",
    "if working_model is not None:\n",
    "    print(\"\\n\\nTesting working model with sentiment prompt...\")\n",
    "    \n",
    "    # The sentiment prompt template\n",
    "    prompt_template = \"\"\"You are a financial sentiment analyzer. Classify the given tweet's sentiment into one of these categories:\n",
    "\n",
    "STRONGLY_POSITIVE - Very bullish, highly confident optimistic outlook\n",
    "POSITIVE - Generally optimistic, bullish view\n",
    "NEUTRAL - Factual, balanced, or no clear sentiment\n",
    "NEGATIVE - Generally pessimistic, bearish view\n",
    "STRONGLY_NEGATIVE - Very bearish, highly confident pessimistic outlook\n",
    "\n",
    "Examples:\n",
    "\"Breaking: Company XYZ doubles profit forecast!\" -> STRONGLY_POSITIVE\n",
    "\"Expecting modest gains next quarter\" -> POSITIVE\n",
    "\"Market closed at 35,000\" -> NEUTRAL\n",
    "\"Concerned about rising rates\" -> NEGATIVE\n",
    "\"Crash incoming, sell everything!\" -> STRONGLY_NEGATIVE\n",
    "\n",
    "Format: Return only one word from: STRONGLY_POSITIVE, POSITIVE, NEUTRAL, NEGATIVE, STRONGLY_NEGATIVE\n",
    "\n",
    "Analyze the sentiment of this tweet: {}\"\"\"\n",
    "    \n",
    "    example_tweets = [\n",
    "        \"Breaking: Company XYZ doubles profit forecast!\",\n",
    "        \"Expecting modest gains next quarter\",\n",
    "        \"Market closed at 35,000\",\n",
    "        \"Concerned about rising rates\",\n",
    "        \"Crash incoming, sell everything!\"\n",
    "    ]\n",
    "    \n",
    "    # Test with the first example\n",
    "    tweet = example_tweets[0]\n",
    "    prompt = prompt_template.format(tweet)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(working_model.device)\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = working_model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.1,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = full_output[len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)):]\n",
    "        \n",
    "        print(f\"\\nTweet: {tweet}\")\n",
    "        print(f\"Model response: '{response}'\")\n",
    "        \n",
    "        if not response.strip():\n",
    "            print(\"\\nModel still not generating a response. Trying with different generation parameters...\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = working_model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_new_tokens=20,  # More tokens\n",
    "                    temperature=0.7,    # Higher temperature\n",
    "                    do_sample=True,     # Enable sampling\n",
    "                    top_p=0.95          # Nucleus sampling\n",
    "                )\n",
    "            \n",
    "            full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = full_output[len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True)):]\n",
    "            \n",
    "            print(f\"Model response with adjusted parameters: '{response}'\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing sentiment prompt: {str(e)}\")\n",
    "else:\n",
    "    print(\"\\nAll model loading options failed. Please check your environment configuration.\")\n",
    "    \n",
    "print(\"\\nAfter you find a working model configuration, update your model loading code accordingly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "TensorBoard logs will be saved to: /workspace/logs/gemma3_training\n",
      "To view training progress, connect to TensorBoard using:\n",
      "  1. In a new terminal: tensorboard --logdir=/workspace/logs/gemma3_training --port=6006\n",
      "  2. Or access through RunPod port forwarding on port 6006\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# We don't need to move the model to device since we're using device_map=\"auto\"\n",
    "# which handles device placement automatically\n",
    "\n",
    "# Define optimizer with weight decay\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 20\n",
    "batch_size = 8  # Smaller batch size due to model size\n",
    "learning_rate = 2e-4  # Lower learning rate for stability\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 100\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Total steps for scheduler\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStoppingCallback(patience=3)\n",
    "\n",
    "log_dir = '/workspace/logs/gemma3_training'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir)\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "print(\"To view training progress, connect to TensorBoard using:\")\n",
    "print(f\"  1. In a new terminal: tensorboard --logdir={log_dir} --port=6006\")\n",
    "print(\"  2. Or access through RunPod port forwarding on port 6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d245cbbd510e44fd9c619d0ef33c497b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.6676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c74c40aa4744710a7ad2c933291de4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7533\n",
      "precision: 0.7065\n",
      "recall: 0.7533\n",
      "f1: 0.6843\n",
      "kappa: 0.2029\n",
      "mcc: 0.2684\n",
      "roc_auc: 0.0000\n",
      "New best model saved!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f40247617049b4b86127100434c18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.4936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e79a9a4d2b4b0babc9510a1cd55a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7775\n",
      "precision: 0.7617\n",
      "recall: 0.7775\n",
      "f1: 0.7644\n",
      "kappa: 0.4365\n",
      "mcc: 0.4426\n",
      "roc_auc: 0.0000\n",
      "New best model saved!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3213c6c9377d4f80bda9a3b1a83582f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.3892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f39bd57d204b6a9589bcaa2a926d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7732\n",
      "precision: 0.7734\n",
      "recall: 0.7732\n",
      "f1: 0.7727\n",
      "kappa: 0.4817\n",
      "mcc: 0.4818\n",
      "roc_auc: 0.0000\n",
      "New best model saved!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4db7939f0c47ae930cf7f4ea847166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.2895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a4e9e586c3422bb0ae46b65fa3271c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7750\n",
      "precision: 0.7528\n",
      "recall: 0.7750\n",
      "f1: 0.7507\n",
      "kappa: 0.3873\n",
      "mcc: 0.4063\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0324222678b0451b8d1c3963ed8985db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.2127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd00a913e0ef4cadb4719314fcac8d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7647\n",
      "precision: 0.7548\n",
      "recall: 0.7647\n",
      "f1: 0.7580\n",
      "kappa: 0.4307\n",
      "mcc: 0.4328\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5d8a2178cd44ec81ae8e81fc09cafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.1604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c8a9ac76794038b12cb7897730de48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7743\n",
      "precision: 0.7613\n",
      "recall: 0.7743\n",
      "f1: 0.7643\n",
      "kappa: 0.4422\n",
      "mcc: 0.4456\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d3412f6af9437181fe09ecf60348b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.1192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810011393a654ccdb88001c5cc006f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7681\n",
      "precision: 0.7629\n",
      "recall: 0.7681\n",
      "f1: 0.7648\n",
      "kappa: 0.4542\n",
      "mcc: 0.4547\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bcdd07afe64c2da7e04ddbc94dfe5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0964\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b7bc86b68d4e9288b2c1e2d1ff3307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7643\n",
      "precision: 0.7564\n",
      "recall: 0.7643\n",
      "f1: 0.7597\n",
      "kappa: 0.4401\n",
      "mcc: 0.4410\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a31234c1945492caefe7f88897fb0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2952b15e6ab6431da2ec68f263ce2cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7686\n",
      "precision: 0.7572\n",
      "recall: 0.7686\n",
      "f1: 0.7611\n",
      "kappa: 0.4385\n",
      "mcc: 0.4404\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3db32170b43499e97c1843f0853181b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0671\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d0df742a914fbda1deb6520a74cfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7702\n",
      "precision: 0.7571\n",
      "recall: 0.7702\n",
      "f1: 0.7611\n",
      "kappa: 0.4375\n",
      "mcc: 0.4401\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebbf70e9d69468186ac3f09f3f81987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0560\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f4c220409341b688f702d74959ad05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7640\n",
      "precision: 0.7595\n",
      "recall: 0.7640\n",
      "f1: 0.7614\n",
      "kappa: 0.4489\n",
      "mcc: 0.4491\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dca866ca0c34509b3b6dfcaea3f2328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0505\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edc5f439e814d7084424dfddfe42596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7665\n",
      "precision: 0.7522\n",
      "recall: 0.7665\n",
      "f1: 0.7570\n",
      "kappa: 0.4247\n",
      "mcc: 0.4279\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda7f07f92eb495a913ab4eb466173e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0408\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d60000fe4964d9694c585f3973ac19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7688\n",
      "precision: 0.7597\n",
      "recall: 0.7688\n",
      "f1: 0.7624\n",
      "kappa: 0.4467\n",
      "mcc: 0.4479\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e84e9fb9c84bb79dc9794e2454731c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d342dec4cf3d4c78a19161eab4a7cb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7675\n",
      "precision: 0.7624\n",
      "recall: 0.7675\n",
      "f1: 0.7640\n",
      "kappa: 0.4546\n",
      "mcc: 0.4551\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5b0e6bb8d444b9819e81bf0041d654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303516cd69c9478287cf610568a41116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7728\n",
      "precision: 0.7638\n",
      "recall: 0.7728\n",
      "f1: 0.7674\n",
      "kappa: 0.4570\n",
      "mcc: 0.4582\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee1eebf8d2d4b6585fb43455a11881b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0247\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f8de86baa04c8b9ec876762e5e6cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7753\n",
      "precision: 0.7651\n",
      "recall: 0.7753\n",
      "f1: 0.7679\n",
      "kappa: 0.4559\n",
      "mcc: 0.4577\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ed7310807744c99c1e74d5c8bbbf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0203\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b4ffc3cd4e4af99d4bd9c3cc209936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7750\n",
      "precision: 0.7628\n",
      "recall: 0.7750\n",
      "f1: 0.7660\n",
      "kappa: 0.4481\n",
      "mcc: 0.4508\n",
      "roc_auc: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8d3b25358447af8a0df1d531f5bdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/20:   0%|          | 0/2818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0165\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdf19893bc246fdabf6b82b5bbdf053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average training loss: 0.0132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1498d53a57054dea8a5cf3721acea47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "accuracy: 0.7702\n",
      "precision: 0.7610\n",
      "recall: 0.7702\n",
      "f1: 0.7644\n",
      "kappa: 0.4498\n",
      "mcc: 0.4510\n",
      "roc_auc: 0.0000\n",
      "\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_metrics = None\n",
    "best_model_state = None\n",
    "training_history = {\"loss\": [], \"val_metrics\": []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_steps = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Log learning rate to TensorBoard\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        writer.add_scalar('Learning_rate', current_lr, epoch * len(train_loader) + epoch_steps)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "    \n",
    "        # Log batch loss periodically\n",
    "        if epoch_steps % 10 == 0:\n",
    "            writer.add_scalar('Loss/train_batch', loss.item(), epoch * len(train_loader) + epoch_steps)\n",
    "    \n",
    "    \n",
    "    avg_loss = total_loss / epoch_steps\n",
    "    training_history[\"loss\"].append(avg_loss)\n",
    "    print(f\"\\nAverage training loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Log epoch-level training loss\n",
    "    writer.add_scalar('Loss/train_epoch', avg_loss, epoch)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_preds.append(outputs.logits.float().cpu().numpy())  # Convert to float32 first\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    predictions = np.vstack(all_preds)\n",
    "    true_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(predictions, true_labels)\n",
    "    training_history[\"val_metrics\"].append(metrics)\n",
    "    \n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "        # Log each validation metric to TensorBoard\n",
    "        writer.add_scalar(f'Metrics/{metric}', value, epoch)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(metrics):\n",
    "        print(\"\\nEarly stopping triggered!\")\n",
    "        break\n",
    "    \n",
    "    # Save best model\n",
    "    if best_metrics is None or metrics['f1'] > best_metrics['f1']:\n",
    "        best_metrics = metrics\n",
    "        # For PEFT models, we save the state_dict of the adapter instead of the entire model\n",
    "        best_model_state = {k: v.clone() for k, v in model.state_dict().items() if \"lora\" in k}\n",
    "        print(\"New best model saved!\")\n",
    "        \n",
    "        # Log model improvement\n",
    "        writer.add_text('Training/best_model_update', f\"New best model at epoch {epoch+1} with f1: {metrics['f1']:.4f}\", epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final model to /workspace/models/gemma3/gemma3_lora_adapter_final\n",
      "Saving best model to /workspace/models/gemma3/gemma3_lora_adapter_best\n",
      "Training history saved to /workspace/models/gemma3/training_history.csv\n",
      "Best metrics saved to /workspace/models/gemma3/metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save final model and metrics\n",
    "output_dir = \"/workspace/models/gemma3\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the adapter files for both final and best models\n",
    "model_path = os.path.join(output_dir, \"gemma3_lora_adapter_final\")\n",
    "best_model_path = os.path.join(output_dir, \"gemma3_lora_adapter_best\")\n",
    "\n",
    "# Save final model\n",
    "print(f\"Saving final model to {model_path}\")\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Save best model if we have one\n",
    "if best_model_state is not None:\n",
    "    print(f\"Saving best model to {best_model_path}\")\n",
    "    # Create directory for best model\n",
    "    os.makedirs(best_model_path, exist_ok=True)\n",
    "    \n",
    "    # Save the best model state\n",
    "    # For PEFT models we need to handle the state dict differently\n",
    "    model.save_pretrained(best_model_path)\n",
    "    tokenizer.save_pretrained(best_model_path)\n",
    "    \n",
    "    # Log model paths to TensorBoard\n",
    "    writer.add_text('Models', f\"Final model: {model_path}\\nBest model: {best_model_path}\", 0)\n",
    "\n",
    "# Save training history\n",
    "history_path = os.path.join(output_dir, \"training_history.csv\")\n",
    "pd.DataFrame([{\n",
    "    \"epoch\": i+1, \n",
    "    \"loss\": loss, \n",
    "    **metrics\n",
    "} for i, (loss, metrics) in enumerate(zip(training_history[\"loss\"], training_history[\"val_metrics\"]))])\\\n",
    "    .to_csv(history_path, index=False)\n",
    "print(f\"Training history saved to {history_path}\")\n",
    "\n",
    "# Save final performance metrics\n",
    "metrics_path = os.path.join(output_dir, \"metrics.csv\")\n",
    "metrics_df = pd.DataFrame([best_metrics])\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"Best metrics saved to {metrics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the fine-tuned model\n",
    "def load_finetuned_model(adapter_path, base_model):\n",
    "    from peft import PeftModel, PeftConfig\n",
    "    \n",
    "    # Load the base model\n",
    "    # Load the base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        device_map=\"auto\",\n",
    "        token=hf_token  # Pass the token for authentication\n",
    "    )\n",
    "    \n",
    "    # Use our custom classification wrapper\n",
    "    model = GemmaForSequenceClassification(base_model, num_labels=5)\n",
    "    \n",
    "    # Load the PEFT adapter\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example of loading and using the model\n",
    "def predict_sentiment(text, model, tokenizer, label_map_reverse):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    return {\n",
    "        \"sentiment\": label_map_reverse[predicted_class],\n",
    "        \"probabilities\": {label_map_reverse[i]: prob.item() for i, prob in enumerate(probabilities[0])}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll uncomment and run this after training is complete\n",
    "\n",
    "# # Reverse the label map for interpretation\n",
    "# label_map_reverse = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# # Load the fine-tuned model\n",
    "# loaded_model = load_finetuned_model(\n",
    "#     adapter_path=\"../models/gemma3/gemma3_lora_adapter\", \n",
    "#     base_model=\"google/gemma-3-12b-pt\"\n",
    "# )\n",
    "# loaded_tokenizer = AutoTokenizer.from_pretrained(\"../models/gemma3/gemma3_lora_adapter\")\n",
    "\n",
    "# # Test with some example tweets\n",
    "# example_tweets = [\n",
    "#     \"Just announced record profits for Q3! Our company is performing exceptionally well.\",\n",
    "#     \"The market is down 2% today, concerning trend continues.\",\n",
    "#     \"No significant changes in our stock price today, trading sideways.\",\n",
    "#     \"Our competitor's latest product launch is worrying for our market position.\",\n",
    "#     \"Just had lunch with friends, the weather is nice today.\"\n",
    "# ]\n",
    "\n",
    "# for tweet in example_tweets:\n",
    "#     result = predict_sentiment(tweet, loaded_model, loaded_tokenizer, label_map_reverse)\n",
    "#     print(f\"\\nTweet: {tweet}\")\n",
    "#     print(f\"Predicted sentiment: {result['sentiment']}\")\n",
    "#     print(\"Probabilities:\")\n",
    "#     for sentiment, prob in sorted(result['probabilities'].items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "#         print(f\"  {sentiment}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
